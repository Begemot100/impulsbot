import os
import docx2txt
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from dotenv import load_dotenv

# 1. –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# 2. –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
DATA_DIR = "/Users/germany/PycharmProjects/PythonProject/PythonProject7/data"

# 3. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ –≤—Å–µ—Ö .docx —Ñ–∞–π–ª–æ–≤
all_text = ""
for filename in os.listdir(DATA_DIR):
    if filename.endswith(".docx"):
        filepath = os.path.join(DATA_DIR, filename)
        print(f"\nüìÑ –ß—Ç–µ–Ω–∏–µ: {filename}")
        all_text += docx2txt.process(filepath) + "\n"

# 4. –†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = splitter.split_text(all_text)
print(f"\nüìö –ü–æ–ª—É—á–µ–Ω–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}")

# 5. –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ FAISS
embedding_model = OpenAIEmbeddings()
vectorstore = FAISS.from_texts(chunks, embedding_model)

# 6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –ª–æ–∫–∞–ª—å–Ω–æ
vectorstore.save_local("faiss_index")
print("‚úÖ –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ –ø–∞–ø–∫—É ./faiss_index")
